#!/usr/bin/env node
'use strict';

var net               = require('net');
var http              = require('http');
var url               = require('url');
var _                 = require('lodash');
var qs                = require('qs');
var async             = require('async');
var bytes             = require('bytes');
var request           = require('request');
var LogstashTransform = require('../lib/logstash_transform');

var upContainers = [];
var Containers = {};
var Streams = {};
var refreshTimeout;

http.globalAgent.maxSockets = Infinity;

function help() {
  console.log('Streams Docker logs to Logstash. Tails Docker logs from remote Docker hosts.');
  console.log('  Usage: ' + argv.$0 + ' --logstash <host:port>');
  process.exit();
};

var argv = require('yargs')
  .default('logstash', 'localhost')
  .argv;

if (argv.help || argv.h) {
  help();
}

function parseHosts(string) {
  return string
    .replace(/^\s+|\s+$/, '')
    .split(/,|\s+/g);
}

function getHosts(fn) {
  if (argv.hosts) {
    fn(null, parseHosts(argv.hosts));
  }
  else {
    var stdin = process.stdin;
    var hosts = '';
    stdin.setEncoding('utf-8');
    stdin.on('readable', function() {
      var chunk;
      while ((chunk = this.read()) !== null) {
        hosts += chunk;
      }
    });
    stdin.on('end', function() {
      fn(null, parseHosts(hosts));
    });
  }
}

function getContainers(host, fn) {
  var url = 'http://' + host + ':2375/containers/json';
  request({
    url: url,
    json: true,
  }, function(err, res) {
    fn(err, res.body);
  });
}

function topContainer(host, containerId, fn) {
  var query = {
    ps_args: 'aux',
  };
  var url = 'http://' + host + ':2375/containers/' + containerId + '/top?' + qs.stringify(query);
  request({
    url: url,
    json: true,
  }, function(err, res) {
    fn(err, res.body);
  });
}

function getLogStream(host, containerId) {
  var query = {
    follow: 1,
    tail: 0,
    stdout: 1,
    timestamps: 1,
    stderr: 1,
  };
  var url = 'http://' + host + ':2375/containers/' + containerId + '/logs?' + qs.stringify(query);
  return request({url: url, json: true});
}

function prepareStreamHash(host, container) {
  var nameTag = container.Image.split('/')[1] || container.Image;
  var parts = nameTag.split(':');
  return {
    host: host,
    name: _.first(container.Names).replace(/^\//, ''),
    image: container.Image,
    app: parts[0],
    tag: parts[1],
    status: container.Status,
    containerId: container.Id,
    created: container.Created,
  };
}

function getLogStreams(fn) {
  var streams = [];
  getAllContainers(function(err, results) {
    async.each(Object.keys(results), function(host, fn) {
      async.each(results[host], function(container, fn) {
        var stream = getLogStream(host, container.Id);
        streams.push(prepareStreamHash(host, container, stream));
        fn();
      }, fn);
    }, function(err, results) {
      fn(err, streams);
    });
  });
}

function handleStreamEnd(stream) {
  console.log('Stream ends');
  console.log(arguments);
  clearTimeout(refreshTimeout);
  refreshContainers();
}

function handleStreamError(stream, error) {
  console.log('Stream errors');
  console.log(arguments);
  clearTimeout(refreshTimeout);
  refreshContainers();
}

function startLogstashStream(infos, s) {
  console.log('start');
  var parts = url.parse('http://' + argv.logstash);
  var logstashConn = net.connect(parts.port, parts.hostname);
  var tags = _.clone(infos);
  s.pipe(new LogstashTransform(tags)).pipe(logstashConn);
  s.on('end', _.partial(handleStreamEnd, s));
  s.on('error', _.partial(handleStreamError, s));
  logstashConn.on('error', function() {
    console.log('Logstash error');
    console.log(arguments);
  });
}

function getTopMessage(status) {
  return [
    '%cpu: ' + status['%CPU'],
    '%mem: ' + status['%MEM'],
    'rss: ' + bytes(parseInt(status.RSS, 10)),
    'vsz: ' + bytes(parseInt(status.VSZ, 10)),
  ].join(', ');
}

function prepareTopHash(host, container, item, titles) {
  var nameTag = container.Image.split('/')[1] || container.Image;
  var parts = nameTag.split(':');
  var hash = {
    host: host,
    containerId: container.Id,
    datetime: Math.round(new Date().getTime() / 1000),
    type: 'top',
    uptime: container.Status,
    app: parts[0],
    tag: parts[1],
  };
  hash = _.extend(hash, _.object(titles, item));
  hash['@message'] = getTopMessage(hash);
  return hash;
}

function sendTopToLogstash(host, container, top, fn) {
  var _parts = url.parse('http://' + argv.logstash);
  var conn = net.connect(_parts.port, _parts.hostname, function() {
    _.map(top.Processes, function(item) {
      var status = prepareTopHash(host, container, item, top.Titles);
      conn.write(JSON.stringify(status) + '\n', 'utf-8');
    });
    conn.end();
  });
  conn.on('finish', fn);
};

function getAllContainers(fn) {
  var results = {};
  getHosts(function(err, hosts) {
    async.each(hosts, function(host, fn) {
      getContainers(host, function(err, containers) {
        results[host] = containers;
        fn(err);
      });
    }, function(err) {
      fn(err, results);
    });
  });
}

function topAllContainers() {
  getAllContainers(function(err, results) {
    _.map(results, function(containers, host) {
      containers.forEach(function(container) {
        topContainer(host, container.Id, function(err, top) {
          sendTopToLogstash(host, container, top, _.noop);
        });
      });
    });
  });
}


function refreshContainers(fn) {
  fn = fn || _.noop;
  var tmpContainers = [];

  getAllContainers(function(err, results) {
    async.each(Object.keys(results), function(host, fn) {
      async.each(results[host], function(container, fn) {
        var key = host + '-' + container.Id;
        tmpContainers.push(key);
        if (!Containers[key]) {
          Containers[key] = prepareStreamHash(host, container);
        }
        fn();
      }, fn);
    }, function(err) {
      var newContainers = _.difference(tmpContainers, upContainers);
      var oldContainers = _.difference(upContainers, tmpContainers);
      upContainers = _.union(upContainers, newContainers);
      console.log('Updated containers');
      console.log(upContainers);
      console.log('OLD');
      console.log(oldContainers);

      // Starts log forward on new conainters
      async.each(newContainers, function(container, fn) {
        var parts = container.split('-');
        var host = parts[0], containerId = parts[1];

        startLogstashStream(Containers[container], getLogStream(host, containerId));

        fn();
      }, function(err) {
        refreshTimeout = setTimeout(function(){ refreshContainers(); }, 3000);
        fn();
      });
    });
  });
}

function initTops() {
  setInterval(topAllContainers, 10000);
  topAllContainers();
}

function main() {
  initTops();
  refreshContainers();
}

//main();

refreshContainers();
